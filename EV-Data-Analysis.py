# -*- coding: utf-8 -*-
"""Untitled32.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14X2zI31X_uua5QcXzou2UIhysd8gEFV6
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb
from xgboost import XGBClassifier

train_df=pd.read_csv('train.csv')
train_df.dtypes
train_df.isnull().sum()
train_df.dtypes
train_df.head()

train_df['Date'] = pd.to_datetime(train_df['Date'])
train_df['Year'] = train_df['Date'].dt.year
train_df['Month'] = train_df['Date'].dt.month
train_df['Quarter'] = train_df['Date'].dt.quarter
train_df['DayOfWeek'] = train_df['Date'].dt.dayofweek

train_df.fillna(0, inplace=True)
train_df['Revenue_per_Unit'] = train_df.apply(
    lambda row: row['Revenue'] / row['Units_Sold'] if row['Units_Sold'] > 0 else 0, axis=1
)
train_df['Discount_Benefit'] = train_df['Discount_Percentage'] * train_df['Revenue_per_Unit']
train_df['Battery_per_Revenue'] = train_df['Battery_Capacity_kWh'] / (train_df['Revenue'] + 1)
train_df['Revenue_Trend'] = train_df.groupby('Month')['Revenue'].transform('mean')
train_df['High_Discount'] = (train_df['Discount_Percentage'] > train_df['Discount_Percentage'].mean()).astype(int)

threshold = train_df['Revenue_per_Unit'].median()
train_df['Purchased'] = (train_df['Revenue_per_Unit'] > threshold).astype(int)

features = [
    'Battery_Capacity_kWh', 'Discount_Percentage', 'Units_Sold', 'Revenue',
    'Revenue_per_Unit', 'Discount_Benefit', 'Battery_per_Revenue', 'Revenue_Trend',
    'High_Discount', 'Year', 'Month', 'Quarter', 'DayOfWeek'
]
cat_features = ['Region', 'Brand', 'Vehicle_Type', 'Fast_Charging_Option']

X = train_df[features + cat_features]
X = pd.get_dummies(X, columns=cat_features, drop_first=True)
y = train_df['Purchased']

poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X)
X_poly = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.columns))



import matplotlib.pyplot as plt
import seaborn as sns

monthly_sales = train_df.groupby(['Year', 'Month'])['Units_Sold'].sum().reset_index()
monthly_sales['YearMonth'] = pd.to_datetime(monthly_sales[['Year', 'Month']].assign(DAY=1))

plt.figure(figsize=(12, 5))
sns.lineplot(data=monthly_sales, x='YearMonth', y='Units_Sold', marker='o')
plt.title("Monthly EV Sales Trend")
plt.ylabel("Units Sold")
plt.xlabel("Date")
plt.grid(True)
plt.show()

stack_data = train_df.groupby(['Region', 'Vehicle_Type'])['Units_Sold'].sum().unstack().fillna(0)
stack_data.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='Set3')
plt.title('Units Sold by Region and Vehicle Type')
plt.ylabel('Units Sold')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

charging_summary = train_df.groupby('Fast_Charging_Option')['Units_Sold'].sum()

charging_summary.plot(kind='bar', color='orange', title='Units Sold by Fast Charging Option')
plt.ylabel('Units Sold')
plt.xticks(rotation=0)
plt.grid(True)
plt.show()

segment_summary = train_df.groupby('Customer_Segment')[['Units_Sold', 'Discount_Percentage']].agg({
    'Units_Sold':'sum',
    'Discount_Percentage':'mean'
})

segment_summary.plot(kind='bar', figsize=(10, 5), title='Sales & Avg Discount by Customer Segment')
plt.ylabel('Value')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

corr = train_df[['Battery_Capacity_kWh', 'Discount_Percentage', 'Units_Sold', 'Revenue']].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='Greens')
plt.title('Correlation Heatmap')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.decomposition import PCA


plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
sns.histplot(train_df['Battery_Capacity_kWh'], kde=True, color='blue')
plt.title('Battery Capacity Distribution')

plt.subplot(1, 2, 2)
sns.histplot(train_df['Discount_Percentage'], kde=True, color='green')
plt.title('Discount Percentage Distribution')
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import hdbscan
import numpy as np
import plotly.express as px

train_df['Revenue_per_Unit'] = train_df.apply(
    lambda row: row['Revenue'] / row['Units_Sold'] if row['Units_Sold'] > 0 else 0,
    axis=1
)

base_num_feats = ['Battery_Capacity_kWh', 'Discount_Percentage', 'Units_Sold', 'Revenue', 'Revenue_per_Unit', 'Year', 'Month']
cat_cols = ['Region', 'Brand', 'Vehicle_Type', 'Fast_Charging_Option']

train_enc = pd.get_dummies(train_df, columns=cat_cols, drop_first=True, dtype=int)
encoded_feats = train_enc.filter(regex='Region_|Brand_|Vehicle_Type_|Fast_Charging_Option_')

cluster_feats = pd.concat([train_enc[base_num_feats], encoded_feats], axis=1)
cluster_feats = cluster_feats.select_dtypes(include='number')

scaler = StandardScaler()
scaled_feats = scaler.fit_transform(cluster_feats)

pca = PCA(n_components=2, random_state=42)
pca_feats = pca.fit_transform(scaled_feats)
pca_df = pd.DataFrame(pca_feats, columns=['PC1', 'PC2'])

fig = px.scatter(
    pca_df, x='PC1', y='PC2',
    title='Before Clustering: PCA Projection of EV Data',
    opacity=0.7
)
fig.show()

min_cluster_sizes = range(5, 51, 5)
best_score = -1
best_size = None
best_labels = None

for size in min_cluster_sizes:
    clusterer = hdbscan.HDBSCAN(min_cluster_size=size, metric='euclidean', cluster_selection_method='eom')
    labels = clusterer.fit_predict(pca_feats)
    mask = labels != -1

    if len(np.unique(labels[mask])) > 1:
        score = silhouette_score(pca_feats[mask], labels[mask])
        print(f"min_cluster_size={size}: Silhouette Score = {score:.4f}")
        if score > best_score:
            best_score = score
            best_size = size
            best_labels = labels
    else:
        print(f"min_cluster_size={size}: Not enough clusters for silhouette score")

print(f"\n✅ Best min_cluster_size: {best_size} with Silhouette Score: {best_score:.4f}")

mask = best_labels != -1
if len(np.unique(best_labels[mask])) > 1:
    silhouette_avg = silhouette_score(pca_feats[mask], best_labels[mask])
    print("✅ Silhouette Score (Final Best Clustering):", silhouette_avg)
else:
    print("⚠️ Not enough clusters to compute silhouette score for final best clustering.")

pca_hdbscan_df = pd.DataFrame(pca_feats, columns=['PC1', 'PC2'])
pca_hdbscan_df['Cluster'] = best_labels.astype(str)

fig = px.scatter(
    pca_hdbscan_df, x='PC1', y='PC2', color='Cluster',
    title=f'After Clustering: HDBSCAN Clustering Visualization (min_cluster_size={best_size})',
    color_discrete_sequence=px.colors.qualitative.Set2
)
fig.show()

import pandas as pd
import plotly.graph_objects as go
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

print(train_df.columns)

features = train_df[['Units_Sold', 'Discount_Percentage']].copy()

features.fillna(0, inplace=True)

scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

kmeans = KMeans(n_clusters=3, random_state=42)
train_df['Cluster'] = kmeans.fit_predict(scaled_features)

cluster_summary = train_df.groupby('Cluster').agg({
    'Units_Sold': 'mean',
    'Discount_Percentage': 'mean'
}).reset_index()

cluster_summary.columns = ['Cluster', 'Units_Sold', 'Discount_Percentage']

fig = go.Figure()

fig.add_trace(go.Bar(
    x=cluster_summary['Cluster'],
    y=cluster_summary['Units_Sold'],
    name='Avg Units Sold',
    marker_color='steelblue'
))

fig.add_trace(go.Bar(
    x=cluster_summary['Cluster'],
    y=cluster_summary['Discount_Percentage'],
    name='Avg Discount %',
    marker_color='lightgreen'
))

fig.update_layout(
    barmode='group',
    title='Average Units Sold and Discount Percentage by Cluster',
    xaxis_title='Cluster',
    yaxis_title='Average Value',
    template='plotly_white'
)

fig.show()

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb

threshold = train_df['Revenue_per_Unit'].median()
train_df['Purchased'] = (train_df['Revenue_per_Unit'] > threshold).astype(int)

X = train_df.drop(columns=['Purchased', 'Revenue', 'Revenue_per_Unit', 'Units_Sold', 'Date', 'Model', 'Customer_Segment'], errors='ignore')
X = pd.get_dummies(X, columns=['Region', 'Brand', 'Vehicle_Type', 'Fast_Charging_Option'], drop_first=True)
y = train_df['Purchased']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
model.fit(X_train, y_train)




y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


import matplotlib.pyplot as plt

xgb.plot_importance(model, max_num_features=15)
plt.show()


params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5, 6],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2, 0.3],
    'min_child_weight': [1, 3, 5],
    'reg_alpha': [0, 0.1, 1],
    'reg_lambda': [1, 1.5, 2]
}
search = RandomizedSearchCV(
    estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    param_distributions=params,
    n_iter=100,
    scoring='f1_weighted',
    cv=5,
    verbose=1,
    n_jobs=-1
)
search.fit(X_train, y_train)
print("Best Accuracy:", search.best_score_)
print("Best Params:", search.best_params_)

best_model = search.best_estimator_
y_pred = best_model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
labels = ['Not Purchased', 'Purchased']
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.4f}\n")
print("Classification Report:")
print(classification_report(y_test, y_pred))
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping


model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  \
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)


early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


history = model.fit(
    X_train_scaled, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=1
)

loss, accuracy = model.evaluate(X_test_scaled, y_test)
print(f"Test Accuracy: {accuracy:.4f}")

y_pred_probs = model.predict(X_test_scaled)
y_pred_classes = (y_pred_probs > 0.5).astype("int32")

from sklearn.metrics import classification_report, confusion_matrix

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_classes))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_classes))

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred_classes)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Purchased', 'Purchased'], yticklabels=['Not Purchased', 'Purchased'])
plt.title('MLP Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

plt.figure(figsize=(10, 4))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()

!pip install catboost
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import HistGradientBoostingClassifier
import pandas as pd

X = train_df.drop(columns=['Purchased', 'Revenue', 'Revenue_per_Unit', 'Units_Sold', 'Date', 'Model', 'Customer_Segment'], errors='ignore')
X = pd.get_dummies(X, columns=['Region', 'Brand', 'Vehicle_Type', 'Fast_Charging_Option'], drop_first=True)
y = train_df['Purchased']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

estimators = [
    ('catboost', CatBoostClassifier(verbose=0, random_state=42)),
    ('lightgbm', LGBMClassifier(random_state=42)),
    ('histgb', HistGradientBoostingClassifier(random_state=42))
]

stacking_clf = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(max_iter=1000, random_state=42),
    cv=5,
    n_jobs=-1,
    passthrough=True
)

stacking_clf.fit(X_train, y_train)

y_pred = stacking_clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"Stacked Ensemble Accuracy: {acc:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt
import numpy as np

y_prob = stacking_clf.predict_proba(X_test)[:, 1]

prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10, strategy='uniform')

plt.figure(figsize=(6, 5))
plt.plot(prob_pred, prob_true, marker='o', label='Stacked Ensemble')
plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')
plt.title('Calibration Curve')
plt.xlabel('Mean Predicted Probability')
plt.ylabel('Fraction of Positives')
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=stacking_clf.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix - Stacked Ensemble")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, accuracy_score

for name, model in stacking_clf.named_estimators_.items():
    y_pred_base = model.predict(X_test)
    acc_base = accuracy_score(y_test, y_pred_base)
    print(f"{name} Accuracy: {acc_base:.4f}")
    print(f"{name} Classification Report:")
    print(classification_report(y_test, y_pred_base))
    print("-" * 50)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, accuracy_score
import numpy as np

y_proba_stack = stacking_clf.predict_proba(X_test)[:,1]

base_models = {
    'CatBoost': stacking_clf.named_estimators_['catboost'],
    'LightGBM': stacking_clf.named_estimators_['lightgbm'],
    'HistGB': stacking_clf.named_estimators_['histgb']
}

plt.figure(figsize=(10,6))

colors = {
    'CatBoost': '#ff9999',
    'LightGBM': '#99ccff',
    'HistGB': '#99ff99',
    'Stacking Ensemble': '#666666'
}

def get_best_accuracy_point(y_true, fpr, tpr, thresholds, y_proba):
    accuracies = []
    for thresh in thresholds:
        y_pred = (y_proba >= thresh).astype(int)
        acc = accuracy_score(y_true, y_pred)
        accuracies.append(acc)
    best_idx = np.argmax(accuracies)
    return fpr[best_idx], tpr[best_idx], accuracies[best_idx]

for name, model in base_models.items():
    y_proba = model.predict_proba(X_test)[:,1]
    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (Area = {roc_auc:.2f})', color=colors[name])

    fpr_best, tpr_best, acc_best = get_best_accuracy_point(y_test, fpr, tpr, thresholds, y_proba)
    plt.scatter(fpr_best, tpr_best, color=colors[name], edgecolor='black', s=100, zorder=5, marker='o')
    plt.text(fpr_best, tpr_best, f' Acc={acc_best:.2f}', fontsize=9, verticalalignment='bottom')

fpr_stack, tpr_stack, thresholds_stack = roc_curve(y_test, y_proba_stack)
roc_auc_stack = auc(fpr_stack, tpr_stack)
plt.plot(fpr_stack, tpr_stack, label=f'Stacking Ensemble (Area = {roc_auc_stack:.2f})', linewidth=3, color=colors['Stacking Ensemble'])

fpr_best_stack, tpr_best_stack, acc_best_stack = get_best_accuracy_point(y_test, fpr_stack, tpr_stack, thresholds_stack, y_proba_stack)
plt.scatter(fpr_best_stack, tpr_best_stack, color=colors['Stacking Ensemble'], edgecolor='black', s=150, zorder=6, marker='X')
plt.text(fpr_best_stack, tpr_best_stack, f' Acc={acc_best_stack:.2f}', fontsize=11, verticalalignment='bottom')

plt.plot([0,1],[0,1],'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves with Best Accuracy Points Marked')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt

region_performance = train_df.groupby('Region')['Purchased'].mean().sort_values(ascending=True)
brand_performance = train_df.groupby('Brand')['Purchased'].mean().sort_values(ascending=True)

fig, axes = plt.subplots(1, 2, figsize=(16, 7), sharex=True)

bars_region = axes[0].barh(region_performance.index, region_performance.values, color='skyblue', alpha=0.7)
axes[0].set_title('Purchase Rate by Region')
axes[0].set_xlabel('Purchase Rate')
axes[0].grid(axis='x', linestyle='--', alpha=0.5)
axes[0].set_xlim(0, 1)
for bar in bars_region:
    width = bar.get_width()
    axes[0].text(width + 0.01, bar.get_y() + bar.get_height() / 2, f'{width:.2%}', va='center')

bars_brand = axes[1].barh(brand_performance.index, brand_performance.values, color='lightgreen', alpha=0.7)
axes[1].set_title('Purchase Rate by Brand')
axes[1].set_xlabel('Purchase Rate')
axes[1].grid(axis='x', linestyle='--', alpha=0.5)
axes[1].set_xlim(0, 1)
for bar in bars_brand:
    width = bar.get_width()
    axes[1].text(width + 0.01, bar.get_y() + bar.get_height() / 2, f'{width:.2%}', va='center')

plt.tight_layout()
plt.show()

train_df.columns.tolist()

plt.figure(figsize=(10, 4))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()

import plotly.graph_objects as go

max_units = monthly_sales['Units_Sold'].max()
optimal_points = monthly_sales[monthly_sales['Units_Sold'] == max_units]

fig6 = px.line(
    monthly_sales,
    x='YearMonth',
    y='Units_Sold',
    title='Monthly EV Sales Trends',
    template='plotly_white',
    markers=True,
    height=500
)
fig6.add_trace(go.Scatter(
    x=optimal_points['YearMonth'],
    y=optimal_points['Units_Sold'],
    mode='markers+text',
    marker=dict(symbol='star', size=15, color='gold'),
    text=['Optimal Month'] * len(optimal_points),
    textposition='top center',
    name='Optimal Timing'
))

fig6.update_layout(
    xaxis_title='Date',
    yaxis_title='Units Sold',
    annotations=[dict(
        text="Identifies optimal timing for marketing campaigns",
        x=0.5, y=1.05,
        showarrow=False, xref="paper", yref="paper"
    )]
)

fig6.show()

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import xgboost as xgb
import hdbscan
import numpy as np

if 'Cluster' not in train_df.columns:
    print("Warning: 'Cluster' column not found in train_df. Re-running HDBSCAN clustering...")

    base_num_feats = ['Battery_Capacity_kWh', 'Discount_Percentage', 'Units_Sold', 'Revenue', 'Revenue_per_Unit', 'Year', 'Month']
    cat_cols = ['Region', 'Brand', 'Vehicle_Type', 'Fast_Charging_Option']
    train_enc = pd.get_dummies(train_df, columns=cat_cols, drop_first=True, dtype=int)
    encoded_feats = train_enc.filter(regex='Region_|Brand_|Vehicle_Type_|Fast_Charging_Option_')
    cluster_feats = pd.concat([train_enc[base_num_feats], encoded_feats], axis=1)
    cluster_feats = cluster_feats.select_dtypes(include='number')
    scaler = StandardScaler()
    scaled_feats = scaler.fit_transform(cluster_feats)
    pca = PCA(n_components=2, random_state=42)
    pca_feats = pca.fit_transform(scaled_feats)

    best_score = -1
    best_size = None
    best_labels = None
    for size in range(5, 51, 5):
        clusterer = hdbscan.HDBSCAN(min_cluster_size=size, metric='euclidean', cluster_selection_method='eom')
        labels = clusterer.fit_predict(pca_feats)
        mask = labels != -1
        if len(np.unique(labels[mask])) > 1:
            score = silhouette_score(pca_feats[mask], labels[mask])
            if score > best_score:
                best_score = score
                best_size = size
                best_labels = labels

    train_df['Cluster'] = best_labels
    print(f"Clusters assigned to train_df with best min_cluster_size={best_size}, Silhouette Score={best_score:.4f}")

print("train_df columns:", train_df.columns)

cluster_summary = train_df.groupby('Cluster').agg({
    'Battery_Capacity_kWh': 'mean',
    'Discount_Percentage': 'mean',
    'Units_Sold': 'mean',
    'Revenue_per_Unit': 'mean'
}).reset_index()

fig1 = go.Figure()
fig1.add_trace(go.Bar(
    x=cluster_summary['Cluster'].astype(str),
    y=cluster_summary['Battery_Capacity_kWh'],
    name='Avg Battery Capacity (kWh)',
    marker_color='steelblue'
))
fig1.add_trace(go.Bar(
    x=cluster_summary['Cluster'].astype(str),
    y=cluster_summary['Discount_Percentage'],
    name='Avg Discount (%)',
    marker_color='lightgreen'
))
fig1.add_trace(go.Bar(
    x=cluster_summary['Cluster'].astype(str),
    y=cluster_summary['Units_Sold'],
    name='Avg Units Sold',
    marker_color='orange'
))
fig1.update_layout(
    barmode='group',
    title='Customer Segments for Targeted Marketing',
    xaxis_title='Cluster',
    yaxis_title='Average Value',
    template='plotly_white',
    height=500,
    annotations=[dict(text="Identifies distinct customer groups for tailored campaigns", x=0.5, y=1.05, showarrow=False, xref="paper", yref="paper")]
)
fig1.show()

train_df['Discount_Bin'] = pd.cut(train_df['Discount_Percentage'], bins=[-1, 5, 10, 15, 20], labels=['0-5%', '5-10%', '10-15%', '15-20%'])

discount_perf = train_df.groupby('Discount_Bin')['Purchased'].mean().reset_index()

plt.figure(figsize=(8,5))
sns.barplot(x='Discount_Bin', y='Purchased', data=discount_perf, palette='Greens_d')
plt.title("Purchase Rate by Discount Percentage")
plt.xlabel("Discount Range")
plt.ylabel("Purchase Rate")
plt.ylim(0,1)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

shap.summary_plot(shap_values, X_test, plot_type="dot")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

purchase_prob = stacking_clf.predict_proba(X_test)[:, 1]
test_df_copy = train_df.loc[X_test.index].copy()

test_df_copy['purchase_prob'] = purchase_prob
inv_df = test_df_copy.groupby(['Region', 'Customer_Segment']).agg({
    'Units_Sold': 'mean',
    'purchase_prob': 'mean'
}).reset_index()
inv_df['Expected_Units_to_Stock'] = inv_df['Units_Sold'] * inv_df['purchase_prob']

plt.figure(figsize=(12, 7))
sns.barplot(data=inv_df, x='Region', y='Expected_Units_to_Stock', hue='Customer_Segment')
plt.title('Expected Units to Stock by Region and Customer Segment (Model-Driven)')
plt.ylabel('Expected Units to Stock')
plt.xlabel('Region')
plt.legend(title='Customer Segment')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

purchase_prob = stacking_clf.predict_proba(X_test)[:, 1]

X_test_copy = X_test.copy()
X_test_copy['purchase_prob'] = purchase_prob

inventory_df = train_df.loc[X_test.index, ['Region', 'Cluster', 'Units_Sold']].copy()
inventory_df['purchase_prob'] = X_test_copy['purchase_prob']

inventory_df['expected_demand'] = inventory_df['purchase_prob'] * inventory_df['Units_Sold']

plt.figure(figsize=(12, 8))
sns.barplot(
    data=inventory_df,
    x='Region',
    y='expected_demand',
    hue='Cluster',
    ci=None,
    palette='Set3'
)
plt.title('Expected EV Demand by Region and Customer Segment')
plt.xlabel('Region')
plt.ylabel('Expected Demand (Predicted Purchase Probability × Units Sold)')
plt.legend(title='Customer Segment')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

test_idx = X_test_copy.index
region_info = train_df.loc[test_idx, ['Region', 'Revenue_per_Unit']].copy()
region_info['purchase_prob'] = X_test_copy['purchase_prob']

region_info['expected_revenue'] = region_info['purchase_prob'] * region_info['Revenue_per_Unit']

region_summary = region_info.groupby('Region')['expected_revenue'].sum().reset_index()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12,6))
sns.barplot(data=region_summary.sort_values('expected_revenue', ascending=False), x='Region', y='expected_revenue', palette='viridis')
plt.xticks(rotation=45)
plt.title('Expected Revenue by Region Based on Predicted Purchases')
plt.ylabel('Expected Revenue')
plt.xlabel('Region')
plt.tight_layout()
plt.show()

